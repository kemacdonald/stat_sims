[
["index.html", "Stats 60: Learning stats via simulation Chapter: 1 Prerequisites", " Stats 60: Learning stats via simulation Stats 60 TAs 2017-01-01 Chapter: 1 Prerequisites "],
["simulation.html", "Chapter: 2 Simulation 2.1 Why simulate data? 2.2 Setting the seed 2.3 Generating samples from probability distributions 2.4 Repetition", " Chapter: 2 Simulation By the end of this chapter, you will be able to: explain the value of simulating data and analysis in R describe the goals/benefits of functional programming use a variety of different methods for generating your own data using R’s built-in random number generators 2.1 Why simulate data? One way to learn statistics is to TODO Simulating data allows you to quickly: Test your statistical intuitions Experiment with new techniques on data where you already know the answers 2.2 Setting the seed By design, simulations in R are generated randomly. But sometimes you want to reproduce a set of simulations to show others (or your future self) what you did. To accomplish this, you need to use the set.seed() function.1 Compare the output of the following two code chunks: c(rnorm(n = 1, mean = 5, sd = 3), rnorm(n = 1, mean = 5, sd = 3)) ## [1] 5.542366 8.345650 set.seed(45) c(rnorm(n = 1, mean = 5, sd = 3), rnorm(n = 1, mean = 5, sd = 3)) ## [1] 6.022399 2.889979 set.seed(45) c(rnorm(n = 1, mean = 5, sd = 3), rnorm(n = 1, mean = 5, sd = 3)) ## [1] 6.022399 2.889979 2.3 Generating samples from probability distributions First argument is always the number of samples to generate n. Then you provide the parameters of the distribution rnorm(n, ...) rpois(n, ...) rbeta(n, ...) runif((n, ...)) rbinom((n, ...)) 2.4 Repetition Often we will want to simulate repeated draws of samples from these probability distributions. One setting is when we want to simulate an experiment with multiple participants who each complete multiple trials. In this case, if our experiment consisted of 60 trials, then each trial could be modeled as a sample from the probability distribution, e.g., rnorm(n = 60, ...), but if we wanted to simulate more participants, say N = 100, then we would have to write rnorm(n = 60, ...) 100 times. That’s way too much typing! Fortunately, R has a built-in function for repeating an expression called replicate(). The replicate() function takes two arguments: n the number of times to repeat the expression expr the expression that we want to repeat For example, we can write one line of code that can generate a sample (any size we choose) from a uniform distribution, take the mean of each sample, and plot those means as a histogram. qplot(replicate(1000, mean(runif(1000, 0, 10))), geom= &quot;histogram&quot;, bins = 30) But what if we want to do this with other types of distributions? Well, we can extend or make a more general version of this function. A quick aside on functional programming. When writing code, the golden rule of functional programming is “Don’t repeat yourself!” If you find yourself writing the same thing more than once, then it is time to turn that code into a function that you can easily use again in future settings. sim_means_plot &lt;- function(n_rep, n_samp, d, size, m = 0, sd = 1, prob = 0.5, min = 0, max = 1, lambda = 1) { sims &lt;- switch(d, normal = replicate(n_rep, mean(rnorm(n = n_samp, m = m, sd = sd))), binomial = replicate(n_rep, mean(rbinom(n = n_samp, size = size, prob = prob))), uniform = replicate(n_rep, mean(runif(n = n_samp, min = min, max = max))), poisson = replicate(n_rep, mean(rpois(n = n_samp, lambda = lambda))) ) %&gt;% as.data.frame() names(sims) &lt;- &quot;value&quot; ggplot(aes(x = value), data = sims) + geom_histogram() } Let’s test the function. sim_means_plot(n_rep = 10000, n_samp = 100, d = &quot;poisson&quot;, lambda = 1) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Pretty cool, right! Now we can simulate data that has been generated from a varitey of probability distributions, take the mean of that data, and visualize the distribution of those means. In statistics, this is called the sampling distribution, and is a foundational concept for hypothesis testing, which will be covered later in the book. Now that we have set up the basic tools in our simulation toolkit, we can write code to improve our statistical intuitions. This is the focus of the rest of this book. If you are interested in the details of the set.seed() function, you can type ?set.seed into the interpreter.↩ "],
["descriptives.html", "Chapter: 3 Descriptive statistics and data vizualization 3.1 Measures of central tendency 3.2 Measures of variability 3.3 Standard scores", " Chapter: 3 Descriptive statistics and data vizualization As an analyst, often our first goal is to describe a data set. What we mean by “describe” here is something like: provide an efficient represenation of the data that is easy for another person to understand. Two effective tools for this task are descriptive statistics and data visualization. It is important to emphasize that the task here is to summarize and communicate with other humans, which means that you need to do more than just figure out how to do the computations. In his statistics book, ognitive scientist, Dan Navarro, has a really nice paragraph on the philosophy of descriptive statistics, so I thought I would include it here: Thus it is no small thing to say that the first task of the statistician and the scientist is to summarise the data, to find some collection of numbers that can convey to an audience a sense of what has happened. This is the job of descriptive statistics, but it’s not a job that can be told solely using the numbers. You are a data analyst, not a statistical software package. Part of your job is to take these statistics and turn them into a description. When you analyse data, it is not sufficient to list off a collection of numbers. Always remember that what you’re really trying to do is communicate with a human audience. The numbers are important, but they need to be put together into a meaningful story that your audience can interpret. That means you need to think about framing. You need to think about context. And you need to think about the individual events that your statistics are summarising With that framing in mind, let’s dive into some common statistical techniques that we can use to efficiently describe our data. 3.1 Measures of central tendency Answers the question: Where are the data? What is the long-run average value of repetitions of the same experiment or data-generating process? 3.1.1 Mean For a data set, the mean provides information about the “central tendency” or the “center of mass” of the data, and is typically denoted using the symbol \\(\\bar{x}\\). Note that if our data set consists of random samples from a larger population, we need to be careful to limit the use of the mean to describe our sample since the population mean ( typically denoted \\(\\mu\\)) is different. To calculate the mean, we take the sum of each value in our data set and then divide by the number of data points. In formal notation this looks like: \\[ \\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} X_i \\] In R, we can quickly compute the mean using the built-in function mean(). Here we are using the Iris data set, which comes with your R installation. The data include a set of measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. m_plength &lt;- mean(d$Petal.Length) m_plength ## [1] 3.758 Note that we could also use our dplyr skills to compute the mean like this. d %&gt;% summarise(m = mean(Petal.Length)) ## m ## 1 3.758 Let’s practice our data viz skills and plot the distribution of petal lengths and include the mean as a vertical dashed line. d %&gt;% ggplot(aes(x = Petal.Length)) + geom_histogram(alpha = 0.7) + geom_vline(xintercept = m_plength, linetype = &quot;dashed&quot;) + annotate(geom = &quot;text&quot;, x = m_plength + .7, y = 20, label = paste(m_plength, &quot;cm&quot;), size = 6) What do we see? Well, it looks like the mean does tell us about the center location of these data. One way to think about this is is that the mean serves as a “balancig point” of the data distribution, with the number to the left of the mean being balanced by the numbers to the right of the mean. But, let’s return to our original goal of providing a useful description of these data. Is m_plength telling us anything useful about these data? Not really. And if we color our plot based on the species of iris, we can see what’s going on here. d %&gt;% ggplot(aes(x = Petal.Length, fill = Species)) + geom_histogram(alpha = 0.7) It looks like there are actually three different distributions of petal length in our data set. We can use our dplyr skills to compute the mean for each distribution separately: ms_petal_length &lt;- d %&gt;% group_by(Species) %&gt;% summarise(m = mean(Petal.Length)) %&gt;% mutate(m = round(m, digits = 2)) And add that information to our plot: d %&gt;% ggplot(aes(x = Petal.Length, fill = Species)) + geom_density(alpha = 0.7) + geom_vline(aes(xintercept = m), data = ms_petal_length, linetype = &quot;dashed&quot;, size = 1) + geom_text(aes(label = paste(m, &quot;cm&quot;), x = m + 0.6, y = 2), data = ms_petal_length, size = 5) I would argue that these three numbers – the mean for each type of iris species in the data set – give us a more useful description of the central tendency. And you can say something like, “The average petal length for the setosa species is 1.46.” 3.1.2 Median 3.1.3 Mode 3.2 Measures of variability Answers the question: How spread out are the data? 3.2.1 Range 3.2.2 Interquartile range 3.2.3 Variance 3.2.4 Standard deviation 3.3 Standard scores "],
["t-test.html", "Chapter: 4 T-test 4.1 The t-test (what, why, and how) 4.2 Comparing samples from two different groups 4.3 Exploring the normality assumption of the t-test", " Chapter: 4 T-test 4.1 The t-test (what, why, and how) The t-test refers to a widely-used set of statistical tools that help us to determine whether two values are “actually” different from each other. (Note the “actually” part will become more clear when we dive into the topic of p-values.) It was developed by William Gosset as a method of statistical quality control to monitor the quality of stout production (there’s a neat backstory that I recommend reading!). 4.1.1 How to compute a t-statistic? The algorithm for computing a t-statistic is as follows: First, we need to determine the null and the alternative hypothesis. This will depend on the type of comparison you want to make. For example, Let’s imagine that you have taken a series of measures on a single group (e.g., NBA players’ heights). You might be interested in testing whether the average height of an NBA player is different from the average height of the general population in the United States. In this scenario, your null hypothesis, \\(H_0\\), is that the average heights are not different from each other, and your alternative hypothesis, \\(H_a\\), is that the two groups are different. (IMPORTANT: here we are just saying that the groups could be different in some way, meaning the NBA players could be shorter OR taller on average.) Second, we need to compute the values that will be in the input to the t-test computation. Here is the formula for a one-sample t-test: \\[t = \\frac{\\bar{x} - u_0} {s / \\sqrt{n}}\\] Let’s code up the one-sample t-test as a function. t_test_fun &lt;- function(measurements, null_val) { xbar &lt;- mean(measurements) s &lt;- sd(measurements) n &lt;- length(measurements) # compute t-statistic (xbar - null_val) / (s / sqrt(n)) } Let’s try our t-test function and compare the output ot R’s built-in t.test() function. # simulate some data as samples from a normal distribution samples &lt;- rnorm(n = 100, mean = 6, sd = 1.5) our_t &lt;- t_test_fun(measurements = samples, null_val = 6) # R&#39;s built-in t-test function Rs_t &lt;- t.test(x = samples, mu = 6)$statistic Our t-statistic is -0.9956524 and R’s is -0.9956524. Nice! Our function worked and returned the expected t-statistic. But a t-statistic by itself is not all that useful in helping us to decide whether there is a difference between our measurements and the null value. In the next section, we will show how to use a t-statistic to make decisions (draw inferences) about our data. 4.1.2 How do we use a t-statistic? To understand how we use a t-statistic, we need to understand the t-distribution. As always Wikipedia provides a useful description: Student’s t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown. It was developed by William Sealy Gosset under the pseudonym Student. Whereas a normal distribution describes a full population, t-distributions describe samples drawn from a full population; accordingly, the t-distribution for each sample size is different, and the larger the sample, the more the distribution resembles a normal distribution. There’s a lot of information in that description! Let’s unpack it. The t-distribution is described by three parameters: the mean (location), the standard deviation (spread), and the degrees of freedom (area in the tails).2 Here’s what the t-distribution looks like for several different values of degrees of freedom. The key parameter for the t-distribution is degrees of freedom (df). Note that a smaller df value creates a distribution with more area in the tails (shorter and wider) than one with a larger df value. Intuitively, this means that you would need a larger t-statistic to achieve the same probability to reject the null hypothesis that the observed mean in our sample is different from the null value. The key insight about the t-distribution is that it provides the expected distribution of t-statistics if we assumed that the null hypothesis was true (that there is no difference between our sample measurments and the population value). Let’s use our simulation skills to test this. t_sims_null &lt;- replicate(n = 1000, t.test(x = rnorm(n = 100, mean = 0, sd = 1), mu = 0)$statistic) %&gt;% data.frame() ggplot(aes(x = .), data = t_sims_null) + geom_line(stat=&quot;density&quot;, size = 1, color = &quot;darkorange&quot;) + geom_line(aes(x = rt(n = 1000, df = 99)), stat = &quot;density&quot;, color = &quot;dodgerblue&quot;, size = 1) Now when we compute a t-statistic for our sample, the question we will ask is: how likely is this t-statistic if we assume that the null hypothesis is true. And since we now have an expected probability distribution of t-statistics, we can compare where our t-value falls in this distribution. Intuitively, if it is towards the center of the probability distribution (where there is a lot of probability mass), then it is more likely that our sample t-value was generated from same process that generated the null distribution. To actually do this in R, we can use the pt() function, which returns the probability of 2*pt(-abs(our_t), df = length(samples) - 1) == t.test(x = samples, mu = 6)$p.value ## [1] TRUE We can also visualize the location of our t-statistic relative to the null t-distribution. ggplot(aes(x = .), data = t_sims_null) + geom_line(stat = &quot;density&quot;, size = 1, color = &quot;darkorange&quot;) + geom_vline(xintercept = our_t, linetype = &quot;dashed&quot;, size = 1) We can see that the location of our t-statistic is not very far out in the tails of the distribution, meaning that it is unlikely that our measurements were generated by a process that was different from the process that generated the collection of t-statistics in the null distribution (if we assume that there is no difference). But what would a t-statistic look like if there was a difference between our sample and the population value? samples2_nba &lt;- rnorm(n = 100, mean = 6.5, sd = 1.5) sig_t &lt;- t.test(x = samples2_nba, mu = 6)$statistic ggplot(aes(x = .), data = t_sims_null) + geom_line(stat = &quot;density&quot;, size = 1, color = &quot;darkorange&quot;) + geom_vline(xintercept = our_t, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1) + geom_vline(xintercept = sig_t, linetype = &quot;dashed&quot;, color = &quot;dodgerblue&quot;, size = 1) 4.2 Comparing samples from two different groups \\[t = \\frac{\\bar{x_1} - \\bar{x_2}} {s_p \\sqrt{2/n}}\\] Where \\(s_p\\) is the pooled standard deviation that we use to etsimate the variance of the two populations that we are sampling from. \\[s_p = \\sqrt{\\frac{s^2_{x_1} + s^2_{x_2}}{2}}\\] Next, let’s write a function that allows us to simulate an experiment where we collect from two different groups and we want to know whether there is likely to be a difference between the two groups. To answer this question, we perform a two-sample, independent t-test on the mean difference between the groups. sim_two_samp_ttest &lt;- function(n_samps, m1, sd1, m2, sd2, paired = FALSE) { group1 &lt;- rnorm(n = n_samps, m = m1, sd = sd1) group2 &lt;- rnorm(n = n_samps, m = m2, sd = sd2) t.test(group1, group2, paired = paired)$p.value } replicate(1000, sim_two_samp_ttest(n_samps = 200, m1 = 5, sd1 = 0.5, m2 = 5.1, sd2 = 0.5)) %&gt;% data.frame() %&gt;% ggplot(aes(.)) + geom_histogram(aes(y = ..density..), bins = 50) + geom_line(aes(y = ..density.., color = &quot;Empirical density&quot;), stat = &#39;density&#39;, size = 1.5) + geom_vline(xintercept = .05, linetype = &quot;dashed&quot;, color = &quot;dodgerblue&quot;, size = 1) + theme(legend.position = &quot;top&quot;) 4.3 Exploring the normality assumption of the t-test The t-test assumes that the underlying process generates data that follows a normal distribution. This makes sense since we have to make these assumptions in order to generate a distribution of expected t-statistics if the null were true. Using simulation we can see what happens to the t-stastic as our data depart from this normality assumption. First, let’s simulate measurements from two groups as samples from two normal distributions with different mean parameters. group1 &lt;- rnorm(n = 500, m = 0, sd = 0.5) group2 &lt;- rnorm(n = 500, m = 0.1, sd = 0.5) Always a good idea to visualize these data. ggplot() + geom_line(aes(group1), stat = &quot;density&quot;, color = &quot;dodgerblue&quot;, size = 1.5) + geom_line(aes(group2), stat = &quot;density&quot;, color = &quot;darkorange&quot;, size = 1.5) What do you think? Are these groups differnt from each other? Tricky, right? Let’s do a t-test to see how likely this difference is compared to the null hypothesis of no difference between the two groups. t.test(x = group1, y = group2, paired = F) ## ## Welch Two Sample t-test ## ## data: group1 and group2 ## t = -2.1399, df = 997.18, p-value = 0.0326 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.129675133 -0.005613819 ## sample estimates: ## mean of x mean of y ## -0.01376446 0.05388002 Formally, degrees of freedom are the number of values in the final calculation of a statistic that are free to vary.↩ "]
]
