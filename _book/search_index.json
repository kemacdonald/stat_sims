[
["index.html", "Stats 60: Learning stats via simulation Chapter: 1 Prerequisites", " Stats 60: Learning stats via simulation Kyle MacDonald 2016-12-19 Chapter: 1 Prerequisites "],
["simulation.html", "Chapter: 2 Simulation", " Chapter: 2 Simulation By the end of this chapter, you will be able to: explain the value of simulating data and analysis in R use a variety of different methods for generating your own data using R’s built-in random number generators 2.0.1 Why simulate data? Two major advantages for simulating data are the ability to: Test your statistical intuitions Experiment with a new technique on known data 2.0.2 Setting the seed By design, simulations in R are generated randomly. But sometimes you want to reproduce a set of simulations. To do this, you need to use the set.seed() function. Compare the output of the following two code chunks: c(rnorm(n = 1, mean = 5, sd = 3), rnorm(n = 1, mean = 5, sd = 3)) ## [1] 5.619496 10.041234 set.seed(45) c(rnorm(n = 1, mean = 5, sd = 3), rnorm(n = 1, mean = 5, sd = 3)) ## [1] 6.022399 2.889979 set.seed(45) c(rnorm(n = 1, mean = 5, sd = 3), rnorm(n = 1, mean = 5, sd = 3)) ## [1] 6.022399 2.889979 2.0.3 Generating samples from probability distributions First argument is always the number of samples to generate n. Then you provide the parameters of the distribution rnorm(n, ...) rpois(n, ...) rbeta(n, ...) runif((n, ...)) rbinom((n, ...)) 2.0.4 Don’t reapeat yourself! The golden rule of functional programming is “Don’t repeat yourself!” If you find yourself writing the same code more than once, then it is time to abstract and make that code into a function. sim_hist_plot &lt;- function(vect, bins = 30) { if (is.data.frame(data) == F) { data &lt;- as.data.frame(vect) } ggplot(aes(x = vect), data = data) + geom_histogram(bins = bins) } sim_hist_plot(rnorm(n = 10000, m = 0, sd = 1), bins = 100) 2.0.5 Repetition To repeat stuff, we can use the replicate() function, which takes two arguments: n the number of times to do the thing expr the thing to do For example, we can write one line of code that can generate a sample (any size we choose) from a uniform distribution, take the mean of each sample, and plot those means as a histogram. qplot(replicate(1000, mean(runif(1000, 0, 10))), geom= &quot;histogram&quot;, bins = 30) But what if we want to do this with other types of distributions? Well, we can extend or make a more general version of this function. sim_means_plot &lt;- function(n_rep, n_samp, d, size, m = 0, sd = 1, prob = 0.5, min = 0, max = 1, lambda = 1) { sims &lt;- switch(d, normal = replicate(n_rep, mean(rnorm(n = n_samp, m = m, sd = sd))), binomial = replicate(n_rep, mean(rbinom(n = n_samp, size = size, prob = prob))), uniform = replicate(n_rep, mean(runif(n = n_samp, min = min, max = max))), poisson = replicate(n_rep, mean(rpois(n = n_samp, lambda = lambda))) ) %&gt;% as.data.frame() names(sims) &lt;- &quot;value&quot; ggplot(aes(x = value), data = sims) + geom_histogram() } Let’s test the function. sim_means_plot(n_rep = 10000, n_samp = 100, d = &quot;poisson&quot;, lambda = 1) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Now that we have set up the basic tools in our simulation toolkit, we can start writing code to improve our statistical intuitions. This is the focus of the rest of this book. "],
["t-test.html", "Chapter: 3 T-test 3.1 The t-test (what, why, and how)", " Chapter: 3 T-test 3.1 The t-test (what, why, and how) The t.test refers to a widely-used set of statistical tools that help us to determine whether two values are “actually” different from each other. (Note the “actually” part will become more clear when we dive into the topic of p-values.) It was developed by William Gosset as a method of statistical quality control to monitor the quality of stout production (there’s a neat backstory that I recommend reading!). 3.1.0.1 How to compute a t-statistic? The algorithm for computing a t-statistic is as follows: First, we need to determine the null and the alternative hypothesis. This will depend on the type of comparison you want to make. For example, Let’s imagine that you have taken a series of measures on a single group (e.g., NBA players’ heights). You might be interested in testing whether the average height of an NBA player is different from the average height of the general population in the United States. In this scenario, your null hypothesis, \\(H_0\\), is that the average heights are not different from each other, and your alternative hypothesis, \\(H_a\\), is that the two groups are different. (IMPORTANT: here we are just saying that the groups could be different in some way, meaning the NBA players could be shorter OR taller on average.) Second, we need to compute the values that will be in the input to the t-test computation. Here is the formula for a one-sample t-test: \\[t = \\frac{\\bar{x} - u_0} {s / \\sqrt{n}}\\] Let’s code up the one-sample t-test as a function. t_test_fun &lt;- function(measurements, null_val) { xbar &lt;- mean(measurements) s &lt;- sd(measurements) n &lt;- length(measurements) # compute t-statistic (xbar - null_val) / (s / sqrt(n)) } Let’s try our t-test function and compare the output ot R’s built-in t.test() function. # simulate some data as samples from a normal distribution samples &lt;- rnorm(n = 100, mean = 6, sd = 1.5) our_t &lt;- t_test_fun(measurements = samples, null_val = 6) # R&#39;s built-in t-test function Rs_t &lt;- t.test(x = samples, mu = 6)$statistic Our t-statistic is 0.4640419 and R’s is 0.4640419. Nice! Our function worked and returned the expected t-statistic. But a t-statistic by itself is not all that useful in helping us to decide whether there is a difference between our measurements and the null value. In the next section, we will show how to use a t-statistic to make decisions (draw inferences) about our data. 3.1.1 How do we use a t-statistic? To understand how we use a t-statistic, we need to understand the t-distribution. As always Wikipedia provides a useful description: Student’s t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arises when estimating the mean of a normally distributed population in situations where the sample size is small and population standard deviation is unknown. It was developed by William Sealy Gosset under the pseudonym Student. Whereas a normal distribution describes a full population, t-distributions describe samples drawn from a full population; accordingly, the t-distribution for each sample size is different, and the larger the sample, the more the distribution resembles a normal distribution. There’s a lot of information in that description! Let’s unpack it. The t-distribution is described by three parameters: the mean (location), the standard deviation (spread), and the degrees of freedom (area in the tails).1 Here’s what the t-distribution looks like for several different values of degrees of freedom. The key parameter for the t-distribution is degrees of freedom (df). Note that a smaller df value creates a distribution with more area in the tails (shorter and wider) than one with a larger df value. Intuitively, this means that you would need a larger t-statistic to achieve the same probability to reject the null hypothesis that the observed mean in our sample is different from the null value. The key insight about the t-distribution is that it provides the expected distribution of t-statistics if we assumed that the null hypothesis was true (that there is no difference between our sample measurments and the population value). Let’s use our simulation skills to test this. t_sims_null &lt;- replicate(n = 1000, t.test(x = rnorm(n = 100, mean = 0, sd = 1), mu = 0)$statistic) %&gt;% data.frame() ggplot(aes(x = .), data = t_sims_null) + geom_line(stat=&quot;density&quot;, size = 1, color = &quot;darkorange&quot;) + geom_line(aes(x = rt(n = 1000, df = 99)), stat = &quot;density&quot;, color = &quot;dodgerblue&quot;, size = 1) Now when we compute a t-statistic for our sample, the question we will ask is: how likely is this t-statistic if we assume that the null hypothesis is true. And since we now have an expected probability distribution of t-statistics, we can compare where our t-value falls in this distribution. Intuitively, if it is towards the center of the probability distribution (where there is a lot of probability mass), then it is more likely that our sample t-value was generated from same process that generated the null distribution. To actually do this in R, we can use the pt() function, which returns the probability of 2*pt(-abs(our_t), df = length(samples) - 1) == t.test(x = samples, mu = 6)$p.value ## [1] TRUE We can also visualize the location of our t-statistic relative to the null t-distribution. ggplot(aes(x = .), data = t_sims_null) + geom_line(stat = &quot;density&quot;, size = 1, color = &quot;darkorange&quot;) + geom_vline(xintercept = our_t, linetype = &quot;dashed&quot;, size = 1) We can see that the location of our t-statistic is not very far out in the tails of the distribution, meaning that it is unlikely that our measurements were generated by a process that was different from the process that generated the collection of t-statistics in the null distribution (if we assume that there is no difference). But what would a t-statistic look like if there was a difference between our sample and the population value? samples2_nba &lt;- rnorm(n = 100, mean = 6.5, sd = 1.5) sig_t &lt;- t.test(x = samples2_nba, mu = 6)$statistic ggplot(aes(x = .), data = t_sims_null) + geom_line(stat = &quot;density&quot;, size = 1, color = &quot;darkorange&quot;) + geom_vline(xintercept = our_t, linetype = &quot;dashed&quot;, color = &quot;black&quot;, size = 1) + geom_vline(xintercept = sig_t, linetype = &quot;dashed&quot;, color = &quot;dodgerblue&quot;, size = 1) 3.1.2 What about comparing samples from two different groups? \\[t = \\frac{\\bar{x_1} - \\bar{x_2}} {s_p \\sqrt{2/n}}\\] Where \\(s_p\\) is the pooled standard deviation that we use to etsimate the variance of the two populations that we are sampling from. \\[s_p = \\sqrt{\\frac{s^2_{x_1} + s^2_{x_2}}{2}}\\] Next, let’s write a function that allows us to simulate an experiment where we collect from two different groups and we want to know whether there is likely to be a difference between the two groups. To answer this question, we perform a two-sample, independent t-test on the mean difference between the groups. sim_two_samp_ttest &lt;- function(n_samps, m1, sd1, m2, sd2, paired = FALSE) { group1 &lt;- rnorm(n = n_samps, m = m1, sd = sd1) group2 &lt;- rnorm(n = n_samps, m = m2, sd = sd2) t.test(group1, group2, paired = paired)$p.value } replicate(1000, sim_two_samp_ttest(n_samps = 200, m1 = 5, sd1 = 0.5, m2 = 5.1, sd2 = 0.5)) %&gt;% data.frame() %&gt;% ggplot(aes(.)) + geom_histogram(aes(y = ..density..), bins = 50) + geom_line(aes(y = ..density.., color = &quot;Empirical density&quot;), stat = &#39;density&#39;, size = 1.5) + geom_vline(xintercept = .05, linetype = &quot;dashed&quot;, color = &quot;dodgerblue&quot;, size = 1) + theme(legend.position = &quot;top&quot;) 3.1.3 Exploring the normality assumption of the t-test The t-test assumes that the underlying process generates data that follows a normal distribution. This makes sense since we have to make these assumptions in order to generate a distribution of expected t-statistics if the null were true. Using simulation we can see what happens to the t-stastic as our data depart from this normality assumption. First, let’s simulate measurements from two groups as samples from two normal distributions with different mean parameters. group1 &lt;- rnorm(n = 500, m = 0, sd = 0.5) group2 &lt;- rnorm(n = 500, m = 0.1, sd = 0.5) Always a good idea to visualize these data. ggplot() + geom_line(aes(group1), stat = &quot;density&quot;, color = &quot;dodgerblue&quot;, size = 1.5) + geom_line(aes(group2), stat = &quot;density&quot;, color = &quot;darkorange&quot;, size = 1.5) What do you think? Are these groups differnt from each other? Tricky, right? Let’s do a t-test to see how likely this difference is compared to the null hypothesis of no difference between the two groups. t.test(x = group1, y = group2, paired = F) ## ## Welch Two Sample t-test ## ## data: group1 and group2 ## t = -3.5539, df = 997.55, p-value = 0.0003974 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.17263356 -0.04980749 ## sample estimates: ## mean of x mean of y ## -0.01268131 0.09853921 Formally, degrees of freedom are the number of values in the final calculation of a statistic that are free to vary.↩ "]
]
